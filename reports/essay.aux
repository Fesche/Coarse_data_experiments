\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{GlobalGreens}
\citation{UNFCCC2015}
\citation{NorwegianMinistryofClimateandEnvironment2014}
\citation{EuropeanCommision2018}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Advanced Metering}{2}{subsection.1.1}}
\citation{Norskelbilforening2018}
\citation{EVvolu}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Power consumption for a house in Texas on a sample day, reported at 15 minute intervals. Although the granularity of the signal seems fine at this resolution, much information may already be lost. Data from the Pecan Street database.\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:consumption_1}{{1}{3}{Power consumption for a house in Texas on a sample day, reported at 15 minute intervals. Although the granularity of the signal seems fine at this resolution, much information may already be lost. Data from the Pecan Street database.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Electric Vehicles}{3}{subsection.1.2}}
\citation{Babrowski2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The growth in the number of privately owned Norwegian EVs has seen a sharp increase over the last decade. Numbers from the Norwegian Bureau of Statistics\relax }}{4}{figure.caption.3}}
\newlabel{fig:norwegian_evs}{{2}{4}{The growth in the number of privately owned Norwegian EVs has seen a sharp increase over the last decade. Numbers from the Norwegian Bureau of Statistics\relax }{figure.caption.3}{}}
\citation{Mullan2012a}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}This report}{5}{subsection.1.3}}
\citation{Hart1992}
\citation{Garey1979}
\citation{RevueltaHerrero2018}
\citation{Kolter2011}
\citation{Kelly2015}
\citation{Pecan2018}
\citation{Kelly2015c,RevueltaHerrero2018}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{6}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Signal Disaggregation and NILM}{6}{subsection.2.1}}
\newlabel{sec:NILM}{{2.1}{6}{Signal Disaggregation and NILM}{subsection.2.1}{}}
\newlabel{eq:disaggregation_formula}{{1}{6}{Signal Disaggregation and NILM}{equation.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The goal of NILM is to disaggregate a power signal into the signals of the individual appliances of which it consists. Appliance 1 is fairly typical of an EV, both in power use and proportion relative to the remaining household consumption.\relax }}{7}{figure.caption.4}}
\newlabel{fig:disagg_1}{{3}{7}{The goal of NILM is to disaggregate a power signal into the signals of the individual appliances of which it consists. Appliance 1 is fairly typical of an EV, both in power use and proportion relative to the remaining household consumption.\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces A selection of relevant results. Note that all of these use sample rates which are considerably higher than those available to a grid operator. In addition to the sample rate, these studies also vary in other factors, such as number of appliances considered. \relax }}{8}{table.caption.5}}
\newlabel{table:lit}{{1}{8}{A selection of relevant results. Note that all of these use sample rates which are considerably higher than those available to a grid operator. In addition to the sample rate, these studies also vary in other factors, such as number of appliances considered. \relax }{table.caption.5}{}}
\citation{Turin1960}
\citation{Zhang2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Approaches}{9}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Pattern Matching}{9}{subsubsection.2.2.1}}
\newlabel{sec:pattern_matching}{{2.2.1}{9}{Pattern Matching}{subsubsection.2.2.1}{}}
\citation{Jia2015}
\newlabel{eq:cross_corr}{{2}{10}{Pattern Matching}{equation.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A filter, constructed by taking the significant parts of the signal of appliance 1, is convolved over the aggregate signal. The output correlation is normalized to the plot, and does not represent any real size.\relax }}{10}{figure.caption.6}}
\newlabel{fig:matched_filter_1}{{4}{10}{A filter, constructed by taking the significant parts of the signal of appliance 1, is convolved over the aggregate signal. The output correlation is normalized to the plot, and does not represent any real size.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Hidden Markov Machines}{10}{subsubsection.2.2.2}}
\newlabel{sec:hmm}{{2.2.2}{10}{Hidden Markov Machines}{subsubsection.2.2.2}{}}
\newlabel{eq:hmm_touple}{{3}{10}{Hidden Markov Machines}{equation.2.3}{}}
\citation{Rumelhart1986}
\citation{Krizhevsky2012}
\citation{Kelly2015c,RevueltaHerrero2018}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Artificial Neural Networks}{11}{subsubsection.2.2.3}}
\newlabel{sec:ann}{{2.2.3}{11}{Artificial Neural Networks}{subsubsection.2.2.3}{}}
\newlabel{eq:ann}{{4}{11}{Artificial Neural Networks}{equation.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces An artificial neural network with one hidden layer. The input consists of two variables; the hidden layer has three nodes with a sigmoid activation (nonlinear) function, and the output layer consists of a single node with no activation. The boxes indicate weights and calculations done during prediction.\relax }}{12}{figure.caption.7}}
\newlabel{fig:ann}{{5}{12}{An artificial neural network with one hidden layer. The input consists of two variables; the hidden layer has three nodes with a sigmoid activation (nonlinear) function, and the output layer consists of a single node with no activation. The boxes indicate weights and calculations done during prediction.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Convolutional Neural Networks}{12}{section*.8}}
\newlabel{eq:conv}{{5}{12}{Convolutional Neural Networks}{equation.2.5}{}}
\citation{Hart1992,Sultanem1991}
\citation{Kelly2015c}
\citation{Kelly2015c}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A single filter sliding over a one-dimensional input. The filter values $f_{00}$ to $f_{03}$ are the trainable weights, and $out_{00}$ to $out_{0n}$ are the filter outputs.\relax }}{13}{figure.caption.9}}
\newlabel{fig:convnet}{{6}{13}{A single filter sliding over a one-dimensional input. The filter values $f_{00}$ to $f_{03}$ are the trainable weights, and $out_{00}$ to $out_{0n}$ are the filter outputs.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Autoencoders}{13}{section*.11}}
\citation{Lecun2015}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A convnet with two convolutional layers and a fully connected layer. The transparent first two rows of squares illustrate the first and second 1x3-filter of the first layer at the first and second steps respectively (note that this means that the layer has a filter step size of 2). The second layer has three channels, one for each filter in the first layer. It also has a single 3x2-filter with a step size of 1, meaning the last hidden layer has four nodes.\relax }}{14}{figure.caption.10}}
\newlabel{fig:convnet_2}{{7}{14}{A convnet with two convolutional layers and a fully connected layer. The transparent first two rows of squares illustrate the first and second 1x3-filter of the first layer at the first and second steps respectively (note that this means that the layer has a filter step size of 2). The second layer has three channels, one for each filter in the first layer. It also has a single 3x2-filter with a step size of 1, meaning the last hidden layer has four nodes.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Recurrent Neural Networks}{14}{section*.13}}
\citation{Kelly2015c}
\citation{Mauch2016}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A minimal autoencoder. The hidden layer creates a lower-dimensional representation of the input; the output layer attempts to recreate the input from the hidden layer's representation. Most AEs will have more layers, and can also use convolutional layers.\relax }}{15}{figure.caption.12}}
\newlabel{fig:autoencoder}{{8}{15}{A minimal autoencoder. The hidden layer creates a lower-dimensional representation of the input; the output layer attempts to recreate the input from the hidden layer's representation. Most AEs will have more layers, and can also use convolutional layers.\relax }{figure.caption.12}{}}
\citation{Dheeru2017}
\citation{Makonin2016}
\citation{Pecan2018}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A simple model of an RNN. At each point in the input sequence, the model takes the gated previous inputs as well as the current input and produces an output as well as the memory for the next step. RNNs can have many layers, but the weights are the same for each step.\relax }}{16}{figure.caption.14}}
\newlabel{fig:rnn}{{9}{16}{A simple model of an RNN. At each point in the input sequence, the model takes the gated previous inputs as well as the current input and produces an output as well as the memory for the next step. RNNs can have many layers, but the weights are the same for each step.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Work Plan}{16}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gathering Data}{16}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Analysis}{17}{subsection.3.2}}
\newlabel{eq:precision}{{6}{17}{Analysis}{equation.3.6}{}}
\newlabel{eq:recall}{{7}{17}{Analysis}{equation.3.7}{}}
\newlabel{eq:F1}{{8}{17}{Analysis}{equation.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Summary and Conclusion}{17}{section.4}}
\bibdata{C:/Users/bfesc/Documents/BibTeX_files/library}
\bibcite{Pecan2018}{1}
\bibcite{Babrowski2014}{2}
\bibcite{Dheeru2017}{3}
\bibcite{Norskelbilforening2018}{4}
\bibcite{EuropeanCommision2018}{5}
\bibcite{EVvolu}{6}
\bibcite{Garey1979}{7}
\bibcite{GlobalGreens}{8}
\bibcite{Hart1992}{9}
\bibcite{Jia2015}{10}
\bibcite{Kelly2015c}{11}
\bibcite{Kelly2015}{12}
\bibcite{Kolter2011}{13}
\bibcite{Krizhevsky2012}{14}
\bibcite{Lecun2015}{15}
\bibcite{Makonin2016}{16}
\bibcite{Mauch2016}{17}
\bibcite{Mullan2012a}{18}
\bibcite{RevueltaHerrero2018}{19}
\bibcite{Rumelhart1986}{20}
\bibcite{Sultanem1991}{21}
\bibcite{NorwegianMinistryofClimateandEnvironment2014}{22}
\bibcite{Turin1960}{23}
\bibcite{UNFCCC2015}{24}
\bibcite{Zhang2011}{25}
\bibstyle{acm}
